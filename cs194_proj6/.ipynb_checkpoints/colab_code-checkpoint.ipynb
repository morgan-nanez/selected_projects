{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in content image\n",
    "content_path = \"content_images/\"\n",
    "content_names = ['neckarfront.jpg']\n",
    "content_image = Image.open(content_path+content_names[0])\n",
    "\n",
    "#load in style image\n",
    "style_path = \"style_images/\"\n",
    "style_names = ['starry_night.jpg']\n",
    "style_image = Image.open(style_path+style_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and apply transforms\n",
    "transform = transforms.Compose([transforms.Resize((512,512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],  \n",
    "                                 [0.229, 0.224, 0.225])])\n",
    "#how to know normalize\n",
    "\n",
    "content_image = transform(content_image).unsqueeze(0) #unsqueeze to make it (1, 1, 512, 512)\n",
    "style_image = transform(style_image).unsqueeze(0)\n",
    "\n",
    "\n",
    "content_image.to(device)\n",
    "style_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSTVgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NSTVgg, self).__init__()\n",
    "        \n",
    "        # load the vgg model's features\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "        # turn  all max poolinglayers into avg pooling\n",
    "        #replace maxpooling layers with avgpooling layer\n",
    "        avgPool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for i, child in self.vgg.named_children():\n",
    "            if isinstance(child, nn.MaxPool2d):\n",
    "                self.vgg[int(i)] = avgPool\n",
    "    \n",
    "    def get_content_feature(self, input):\n",
    "\n",
    "        content_feature = self.vgg[:23](input)\n",
    "        return content_feature\n",
    "    \n",
    "    def get_style_features(self, input):\n",
    "        style_indices = [2,7, 12, 21, 30]\n",
    "        style_feature_layers = []\n",
    "        for i in style_indices:\n",
    "            style_feature_layers.append(self.vgg[:i])\n",
    "        \n",
    "        style_features = [layer(input) for layer in style_feature_layers]\n",
    "\n",
    "\n",
    "        return style_features\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.vgg(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = NSTVgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create white noise input\n",
    "mu, sigma = 0, 1 # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, size=(1, 3, 512, 512))\n",
    "white_noise = torch.FloatTensor(s)\n",
    "white_noise.requires_grad = True\n",
    "white_noise.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lock gradient because its a stationary feature extractor\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get content and style images to compare  for loss\n",
    "content_loss_feature = vgg.get_content_feature(content_image)\n",
    "            \n",
    "            \n",
    "style_loss_features = vgg.get_style_features(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom content and style losses\n",
    "\n",
    "# Code adapted from https://www.javatpoint.com/pytorch-gram-matrix-for-style-transferring\n",
    "#Initializing gram_matrix function for our tensor image   \n",
    "def gramMatrix(tensor):  \n",
    "    _,d,h,w=tensor.size()  \n",
    "    \n",
    "    #Reshaping data into a two dimensional of array or two dimensional of tensor  \n",
    "    tensor=tensor.view(d,h*w) \n",
    "    \n",
    "    gram = torch.mm(tensor,tensor.t())  \n",
    "  \n",
    "    #Returning gram matrix   \n",
    "    return gram  \n",
    "\n",
    "\n",
    "def contentLoss(noise):\n",
    "    #use squared-error loss\n",
    "    #loss = F.mse_loss(noise, content_loss_feature) *  content_weight\n",
    "    loss = 0.5 * torch.sum(torch.pow(noise - content_loss_feature, 2)) *  content_weight\n",
    "\n",
    "    return loss\n",
    "\n",
    "gram_style_targets = [gramMatrix(style_feat) for style_feat in style_loss_features]\n",
    "\n",
    "def styleLoss(noise_layers):\n",
    "    total_style_loss = 0\n",
    "        \n",
    "    # get contribution of each layer to the total loss\n",
    "    for i in range(len(noise_layers)):\n",
    "        gram_noise = gramMatrix(noise_layers[i])\n",
    "        #loss = F.mse_loss(gram_noise, gram_style_targets[i])\n",
    "        loss = 0.5 * torch.sum(torch.pow(gram_noise - gram_style_targets[i], 2)) *  style_weight\n",
    "\n",
    "\n",
    "        total_style_loss += loss * (1/5)\n",
    "        \n",
    "    return torch.sum(total_style_loss) * style_weight\n",
    "    \n",
    "    \n",
    "    \n",
    "# Cod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight = 10e3 #beta  [10e3 or 10e4]\n",
    "content_weight = 1 #alpha\n",
    "num_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.to_device.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.LBFGS([white_noise.requires_grad_(True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#style transfer loop\n",
    "for num in range(num_iter):\n",
    "    print(num)\n",
    "    \n",
    "    #required for lbfgs\n",
    "    def closure():\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #get current content layer and apply to noise image\n",
    "        content_feature = vgg.get_content_feature(white_noise)\n",
    "\n",
    "        #get content loss\n",
    "        \n",
    "        cl = contentLoss(content_feature)\n",
    "        #print(cl)\n",
    "        \n",
    "        \n",
    "\n",
    "        #get current style layers and apply them\n",
    "        style_features = vgg.get_style_features(white_noise)\n",
    "\n",
    "        #get style loss\n",
    "        sl = styleLoss(style_features)\n",
    "        # print(sl)\n",
    "\n",
    "        #get total loss\n",
    "        loss = cl + sl\n",
    "        loss.to(device)\n",
    "       # loss = nn.Parameter(loss_value, requires_grad = True)\n",
    "\n",
    "        #back propagate loss\n",
    "        loss.backward()\n",
    "        #print(loss)\n",
    "        \n",
    "        if num % 10 == 0:\n",
    "            print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}\".format(num, cl, sl, loss))\n",
    "\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # save intermediate  image\n",
    "    if num % 10 == 0:\n",
    "        save_image(white_noise.cpu().detach(), fp='./generated/iter_{}.png'.format(num))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
